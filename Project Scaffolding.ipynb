{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 139,
>>>>>>> Stashed changes
   "id": "66dcde05",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mPython 3.8.12 需要安装 ipykernel。\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -p /Users/yangzhaoqi/opt/anaconda3/envs/yangzq ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import math\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as db\n",
    "# import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "TAXI_ZONES = \"taxi_zones.shp\"\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what you’re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "From latitude and longitude to caluclate the distance. We refer to the formular from: https://en.wikipedia.org/wiki/Geographical_distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord: pd.core.frame.DataFrame, to_coord: pd.core.frame.DataFrame) -> pd.core.series.Series:\n",
    "    \"\"\"Caluclate the distance from latitude and longitude.\n",
    "\n",
    "    Keyword arguments:\n",
    "    from_coord -- A dataframe including 'pickup_latitude' and 'pickup_longitude'\n",
    "    to_coord -- A dataframe including 'dropoff_latitude' and 'dropoff_longitude'\n",
    "\n",
    "    Output:\n",
    "    A series of the distances (km).\n",
    "    \"\"\"\n",
    "\n",
    "    # convert different degrees into radians\n",
    "    pickup_larad = from_coord['pickup_latitude'].apply(math.radians)\n",
    "    pickup_lorad = from_coord['pickup_longitude'].apply(math.radians)\n",
    "    dropoff_larad = to_coord['dropoff_latitude'].apply(math.radians)\n",
    "    dropoff_lorad = to_coord['dropoff_longitude'].apply(math.radians)\n",
    "\n",
    "    # calculate the distance\n",
    "    R = 6371 \n",
    "    la_dif = pickup_larad - dropoff_larad\n",
    "    lo_dif = pickup_lorad - dropoff_lorad\n",
    "    a = ((la_dif / 2).apply(math.sin)) ** 2 + pickup_larad.apply(math.cos) * dropoff_larad.apply(math.cos) * ((lo_dif / 2).apply(math.sin) ** 2)\n",
    "    distance = 2 * R * a.apply(math.sqrt).apply(math.asin)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75b6aa",
   "metadata": {},
   "source": [
    "Add the distance information to the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    '''Add the distance information to the original dataframe.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    df -- A dataframe with columns \"pickup_latitude\" , \"pickup_longitude\",  \"dropoff_latitude\" and \"dropoff_longitude\".\n",
    "\n",
    "    Output:\n",
    "    A dataframe with a new column \"distance\"\n",
    "    '''\n",
    "\n",
    "    from_pos = dataframe[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_pos = dataframe[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    dataframe['distance'] = calculate_distance(from_pos, to_pos)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls(taxi_url = TAXI_URL) -> list:\n",
    "    '''\n",
    "    Go get the URLs of parquet file for taxi data.\n",
    "    Returns A list of urls that point to the parquet files.\n",
    "    '''\n",
    "    response = requests.get(taxi_url)\n",
    "    html = response.content\n",
    "    taxisoup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    links = taxisoup.find_all(\"a\", href=True)\n",
    "\n",
    "    urllist = []\n",
    "    for link in links:\n",
    "        if re.search(r\"yellow_tripdata_2009|yellow_tripdata_201[01234]|yellow_tripdata_2015-0[1-6]\", link.get(\"href\")):\n",
    "            urllist.append(link.get(\"href\"))\n",
    "        # Regular expression to find the links that match the pattern 2009-01~2015-06\n",
    "    return urllist\n",
    "# myurl = find_taxi_parquet_urls()\n",
    "# print(myurl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Download_data(localurl: str)-> pd.core.frame.DataFrame:    \n",
    "    '''\n",
    "    Download the data to local file path if there is non exist.\n",
    "    url -- a string which is the url of the parquet file.\n",
    "    return a dataframe that can we can further work on. \n",
    "    '''\n",
    "    # Get the file name with given url\n",
    "    filename = localurl.split('/')[-1]\n",
    "    # download the file if it doesn't exist\n",
    "    file = requests.get(localurl)\n",
    "    with open(filename , \"wb\") as f:\n",
    "        f.write(file.content)\n",
    "    # load the data with read_parquet\n",
    "    df = pd.read_parquet(filename)\n",
    "    #df.to_csv(filename+\".csv\")\n",
    "    return df\n",
    "# Download_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet')\n",
    "\n",
    "def get_taxi_zone_df() -> gpd.geodataframe.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Get a GeoDataFrame\n",
    "    \"\"\"\n",
    "    taxi_zone_df = gpd.read_file(filename='taxi_zones.zip', engine='fiona')\n",
    "    # taxi_zone_df = gpd.read_file(TAXI_ZONES)\n",
    "    taxi_zone_df = taxi_zone_df.to_crs(4326)\n",
    "    taxi_zone_df['longitude'] = taxi_zone_df.centroid.x\n",
    "    taxi_zone_df['latitude'] = taxi_zone_df.centroid.y\n",
    "    taxi_zone_df.drop_duplicates('LocationID', inplace = True)\n",
    "    taxi_zone_df.set_index('LocationID', inplace = True)\n",
    "    return taxi_zone_df\n",
    "# mypandas = get_taxi_zone_df()\n",
    "# print(type((mypandas)))\n",
    "\n",
    "\n",
    "def Clean_month_taxi_data(url: str, GEOmetry: gpd.geodataframe.GeoDataFrame) -> pd.core.frame.DataFrame:\n",
    "    '''\n",
    "    Download the data to local and Clean the taxi data for a given month.\n",
    "    url -- a string which is the url of the parquet file.\n",
    "    Returns A pandas dataframe\n",
    "    '''\n",
    "    # Call download_data fucntion to download data from url and return csv\n",
    "    df = Download_data(localurl = url)\n",
    "    print(\"cleaning data from\", url)\n",
    "    # looking up the latitude and longitude for some months where only location IDs are given for pickups and dropoffs\n",
    "    if 'PULocationID' in df.columns:\n",
    "        df['pickup_latitude'] = df['PULocationID'].map(GEOmetry['latitude'])\n",
    "        df['pickup_longitude'] = df['PULocationID'].map(GEOmetry['longitude'])\n",
    "        df['dropoff_latitude'] = df['DOLocationID'].map(GEOmetry['latitude'])\n",
    "        df['dropoff_longitude'] = df['DOLocationID'].map(GEOmetry['longitude'])\n",
    "    # Get rid of the NA's, which indicate a poor quality of the data.\n",
    "    df.dropna()\n",
    "    # normalizing column names\n",
    "    df.rename(columns={'tpep_pickup_datetime':'pickup_datetime','\\\n",
    "        Trip_Pickup_DateTime':'pickup_datetime',\\\n",
    "        'Trip_Dropoff_DateTime':'dropoff_datetime',\\\n",
    "        'tpep_dropoff_datetime':'dropoff_datetime',\\\n",
    "        'Start_Lon':'pickup_longitude', \\\n",
    "        'Start_Lat':'pickup_latitude', \\\n",
    "        'End_Lon':'dropoff_longitude', \\\n",
    "        'End_Lat':'dropoff_latitude',\n",
    "        'Tip_Amt':'tip_amount'}, inplace=True)\n",
    "    # normalizing and using appropriate column types for the respective data\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "\n",
    "    # removing trips that start and/or end outside of the NEW_YORK_BOX_COORDS\n",
    "    df = df[(df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "    df = df[(df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "\n",
    "\n",
    "    # leaving only needed data columns, by selecting what's needed\n",
    "    df = df[['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_datetime','dropoff_longitude','dropoff_latitude','tip_amount']]\n",
    "\n",
    "    # sample the data to make it roughly equal to the size of the Uber dataset\n",
    "    # ~200000 is the number of rows in the Uber dataset\n",
    "    # so every month we sample 200000 / 78 ~ 2500 rows\n",
    "    df = df.sample(n=2500)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    all_urls = find_taxi_parquet_urls(TAXI_URL)\n",
    "    print(all_urls[0])\n",
    "    GEOmetry = get_taxi_zone_df()\n",
    "    count = 1\n",
<<<<<<< Updated upstream
    "    # for one_url in all_urls:\n",
    "    for one_url in [all_urls[0], all_urls[1]]:\n",
=======
    "    for one_url in all_urls:\n",
    "    # for one_url in [all_urls[0], all_urls[1]]:\n",
>>>>>>> Stashed changes
    "        print(one_url)\n",
    "        print(\"processing dataset\" + str(count))\n",
    "        dataframe = Clean_month_taxi_data(one_url, GEOmetry)\n",
    "        # add_distance_column(dataframe)\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_uber_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.drop(columns=[\"Unnamed: 0\",\"key\"])\n",
    "    df = df.dropna()\n",
    "    \n",
    "    #pick up in bounding box \n",
    "    df = df[(df['pickup_longitude']> NEW_YORK_BOX_COORDS[0][1]) &  (df['pickup_longitude']<NEW_YORK_BOX_COORDS[1][1])]\n",
    "    df = df[(df['pickup_latitude']> NEW_YORK_BOX_COORDS[0][0]) &  (df['pickup_latitude']<NEW_YORK_BOX_COORDS[1][0])]\n",
    "    #drop off in bounding box  \n",
    "    df = df[(df['dropoff_longitude']> NEW_YORK_BOX_COORDS[0][1]) &  (df['dropoff_longitude']<NEW_YORK_BOX_COORDS[1][1])]\n",
    "    df = df[(df['dropoff_latitude']> NEW_YORK_BOX_COORDS[0][0]) &  (df['dropoff_latitude']<NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df = df[['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "Load and clean the hourly windspeed and precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    '''Load the weather data and clean it hourly.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    csv_file -- The name of the raw data csv file.\n",
    "\n",
    "    Output:\n",
    "    A dataframe of the cleaned hourly weather data.\n",
    "    '''\n",
    "\n",
    "    # we only need hourly windspeed and precipitation\n",
    "    df = pd.read_csv(csv_file, usecols=['DATE','HourlyWindSpeed','HourlyPrecipitation'])\n",
    "    \n",
    "    # process date type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "    # deal with missing values\n",
    "    df.dropna(subset=['HourlyWindSpeed'], inplace=True)\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104e81f",
   "metadata": {},
   "source": [
    "Load and clean the daily windspeed and precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    '''Load the weather data and clean it daily.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    csv_file -- The name of the raw data csv file.\n",
    "\n",
    "    Output:\n",
    "    A dataframe with the cleaned daily weather data.\n",
    "    '''\n",
    "\n",
    "    # we only need daily windspeed and precipitation\n",
    "    df = pd.read_csv(csv_file, usecols=['DATE','HourlyWindSpeed','HourlyPrecipitation'])\n",
    "\n",
    "    # deal with missing values\n",
    "    df.dropna(subset=['HourlyWindSpeed'], inplace=True)\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "\n",
    "    # group by day and calculate mean(HourlyWindSpeed) and sum(HourlyPrecipitation)\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    daily_wind_speed = df[['DATE', 'HourlyWindSpeed']].groupby('DATE').mean()\n",
    "    daily_precipitation = df[['DATE', 'HourlyPrecipitation']].groupby('DATE').sum()\n",
    "    daily_wind_speed.rename(columns = {'HourlyWindSpeed':'DailyWindSpeed'}, inplace = True)\n",
    "    daily_precipitation.rename(columns= {'HourlyPrecipitation':'DailyWindSpeed'}, inplace = True)\n",
    "    \n",
    "    return pd.concat([daily_wind_speed, daily_precipitation], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde71eeb",
   "metadata": {},
   "source": [
    "Load the sunrise and sunset time of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e9ed72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_sun_data_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    '''Load the sunrise and sunset time of each day.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    csv_file -- The name of the raw data csv file.\n",
    "\n",
    "    Output:\n",
    "    A dataframe with the sunrise and sunset time of each day.\n",
    "    '''\n",
    "\n",
    "    # load data\n",
    "    df = pd.read_csv(csv_file, usecols=['DATE','Sunrise','Sunset'])\n",
    "    \n",
    "    # process date type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    df.drop('DATE', axis=1, inplace=True)\n",
    "\n",
    "    # deal with redundant data\n",
    "    df.dropna(subset=['Sunrise'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aaa25b",
   "metadata": {},
   "source": [
    "Load and clean all weather and sun data from 2009 to 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    sun_dataframes = []\n",
    "    \n",
    "    # add the name/paths manually\n",
    "    weather_csv_files = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\",\n",
    "                         \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        csv_path = 'weather/'\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_path + csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_path + csv_file)\n",
    "        sun_dataframe = clean_month_sun_data_daily(csv_path + csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        sun_dataframes.append(sun_dataframe)\n",
    "        \n",
    "    # create three dataframes with hourly, daily data and sun from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    sun_data = pd.concat(sun_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data, sun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-144-a7c2e61a2420>:26: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zone_df['longitude'] = taxi_zone_df.centroid.x\n",
      "<ipython-input-144-a7c2e61a2420>:27: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zone_df['latitude'] = taxi_zone_df.centroid.y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet\n",
      "processing dataset1\n",
      "cleaning data from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet\n",
      "processing dataset1\n",
      "cleaning data from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/weather/2009_weather.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-1e1504741db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtaxi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_and_clean_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0muber_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_uber_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhourly_weather_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaily_weather_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaily_sun_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_clean_weather_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-e368324024fd>\u001b[0m in \u001b[0;36mload_and_clean_weather_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweather_csv_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/weather/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mhourly_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_month_weather_data_hourly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdaily_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_month_weather_data_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msun_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_month_sun_data_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-ba3f99486496>\u001b[0m in \u001b[0;36mclean_month_weather_data_hourly\u001b[0;34m(csv_file)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# we only need hourly windspeed and precipitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'HourlyWindSpeed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'HourlyPrecipitation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# process date type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/weather/2009_weather.csv'"
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_data, daily_data, sun_data = load_and_clean_weather_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \\\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    HourlyWindSpeed FLOAT,\n",
    "    HourlyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA =\\\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    DailyWindSpeed FLOAT,\n",
    "    DailyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \\\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    dropoff_datetime DATE,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA =\\\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "SUN_SCHEMA = \\\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sun_data (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    Sunrise INT,\n",
    "    Sunset INT\n",
    ");\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(SUN_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    sql = open(DATABASE_SCHEMA_FILE, \"r\")\n",
    "    commands = sql.read().split(\";\")\n",
    "    sql.close()\n",
    "    for command in commands:\n",
    "        connection.execute(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: dict):\n",
    "    \"\"\"\n",
    "    high\n",
    "    \"\"\"\n",
    "    for i, j in table_to_df_dict.items():\n",
    "        j.to_sql(i,engine, if_exists = 'append', index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "    \"sun_data\": sun_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "d3c6d880",
=======
   "id": "23493a86",
>>>>>>> Stashed changes
   "metadata": {},
   "source": [
    "Write SQL codes to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query: str, outfile: str):\n",
    "    '''Write SQL codes to file.\n",
    "\n",
    "    Keyword arguments:\n",
    "    query -- The SQL query.\n",
    "    outfile -- The name of the output file.\n",
    "    '''\n",
    "\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "For 01-2009 through 06-2015, what hour of the day was the most popular to take a Yellow Taxi? The result should have 24 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "    SELECT strftime('%H', pickup_datetime) AS hour, \n",
    "           COUNT(*) AS trip_count\n",
    "    FROM taxi_trips\n",
    "    GROUP BY hour\n",
    "    ORDER BY trip_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"hour_trend_taxi.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750206e",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "For the same time frame, what day of the week was the most popular to take an Uber? The result should have 7 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "    SELECT strftime('%w', pickup_datetime) AS day, \n",
    "           COUNT(*) AS trip_count\n",
    "    FROM uber_trips\n",
    "    GROUP BY day\n",
    "    ORDER BY trip_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a47aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"week_trend_uber.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6c4e1",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "What is the 95% percentile of distance traveled for all hired trips during July 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d153868",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "    WITH all_trips AS (SELECT pickup_datetime, distance \n",
    "                       FROM taxi_trips \n",
    "                       WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31'\n",
    "                       UNION ALL\n",
    "                       SELECT pickup_datetime, distance \n",
    "                       FROM uber_trips\n",
    "                       WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31')\n",
    "    SELECT distance AS '95% percentile distance'\n",
    "    FROM all_trips\n",
    "    ORDER BY distance ASC\n",
    "    LIMIT 1\n",
    "    OFFSET (SELECT COUNT(*) \n",
    "            FROM all_trips) * 95 / 100 - 1 ;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95_percentile_distance.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcc225",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "    WITH all_trips AS (SELECT pickup_datetime, distance \n",
    "                       FROM taxi_trips \n",
    "                       WHERE pickup_datetime BETWEEN '2009-01-01' AND '2009-12-31'\n",
    "                       UNION ALL\n",
    "                       SELECT pickup_datetime, distance \n",
    "                       FROM uber_trips\n",
    "                       WHERE pickup_datetime BETWEEN '2009-01-01' AND '2009-12-31')\n",
    "    SELECT date(pickup_datetime) AS date, AVG(distance) AS avg_distance, COUNT(*) AS trip_count\n",
    "    FROM all_trips\n",
    "    GROUP BY date\n",
    "    ORDER BY trip_count DESC\n",
    "    LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c19dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"10_days_most_trips.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4f5c6",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "Which 10 days in 2014 were the windiest on average, and how many hired trips were made on those days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d561e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "    WITH all_trips AS (SELECT pickup_datetime \n",
    "                       FROM taxi_trips\n",
    "                       UNION ALL\n",
    "                       SELECT pickup_datetime \n",
    "                       FROM uber_trips)\n",
    "    SELECT date(pickup_datetime) AS date, COUNT(*) AS trip_count, \n",
    "    FROM all_trips\n",
    "    GROUP BY date\n",
    "    HAVING date IN (SELECT date(DATE) \n",
    "                    FROM daily_weather \n",
    "                    WHERE DATE BETWEEN '2014-01-01' AND '2014-12-31' \n",
    "                    ORDER BY DailyWindSpeed DESC \n",
    "                    LIMIT 10);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ffcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df084fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"10_day_windiest.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04373b9f",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "During Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8255f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "    WITH all_trips AS (SELECT strftime('%m/%d/%Y %H', pickup_datetime) as trip_date_hour\n",
    "                       FROM taxi_trips\n",
    "                       WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-04'\n",
    "                       UNION ALL\n",
    "                       SELECT strftime('%m/%d/%Y %H', pickup_datetime) as trip_date_hour \n",
    "                       FROM uber_trips\n",
    "                       WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-04')\n",
    "    SELECT strftime('%m/%d/%Y %H', DATE) AS weather_date_hour\n",
    "           COALESCE(COUNT(all_trips.trip_date_hour), 0) AS trip_count, \n",
    "           HourlyPrecipitation, \n",
    "           HourlyWindSpeed\n",
    "    FROM hourly_weather\n",
    "    LEFT JOIN all_trips\n",
    "    ON weather_date_hour = trip_date_hour\n",
    "    WHERE weather_date_hour BETWEEN '2012-10-22' AND '2012-11-04'\n",
    "    GROUP BY weather_date_hour\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f55df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"Sandy_weather_trip.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28a3d8d1478415082f07ed9619fcf332666a33d26ded38c8b5f99e4a8762da54"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
